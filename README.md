# Tecnvi Realtime AI Backend
**WebSockets â€¢ LLM Streaming â€¢ Supabase Persistence**

This repository implements the assignment **â€œRealtime AI Backend (WebSockets + Supabase)â€** using **FastAPI**, **WebSockets**, and **Supabase Postgres**.

The system demonstrates real-time AI response streaming, complex multi-step interaction patterns, persistent session storage, and post-session AI processing.

---

## Key Features

- **Realtime bi-directional WebSocket sessions**  
  Endpoint: `/ws/session/{session_id}`

- **Token-level streaming**  
  AI responses are streamed incrementally to simulate low-latency LLM output.

- **Complex LLM interaction patterns**
  - Multi-turn conversation state management
  - Tool / function-calling architecture (simulated internal tools)
  - Conditional follow-up questions when required parameters are missing

- **Supabase Postgres persistence**
  - Session metadata table
  - Detailed chronological event log

- **Post-session processing**
  - On WebSocket disconnect, an asynchronous job analyzes the conversation
  - Generates a concise session summary
  - Persists end time, duration, and summary

- **Lightweight frontend**
  - Simple HTML/CSS/JavaScript UI
  - Focused on WebSocket connectivity and streaming (UI/UX intentionally minimal)

---

## LLM Usage (Important)

This project supports **two execution modes**:

### 1. Mock LLM Mode (Default)
If `OPENAI_API_KEY` is **not set**:
- AI responses are generated by a mock LLM layer
- Token streaming is simulated
- Tool calls are simulated
- Post-session summarization is deterministic

This mode allows:
- Offline execution
- Cost-free evaluation
- Easy reviewer testing

### 2ï¸. Real LLM Mode (Optional)
If `OPENAI_API_KEY` **is set**:
- A real LLM is used for:
  - Streaming responses
  - Tool / function calling
  - Post-session summarization

Switching between mock and real LLM **requires no code changes**, only environment configuration.

> The architecture is identical in both modes.

---

## 1ï¸. Setup

### Prerequisites
- Python **3.10+**
- A **Supabase** project with Postgres enabled
- (Optional) OpenAI API key for real LLM execution

---

### Installation

```bash
python -m venv .venv
source .venv/bin/activate    # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

---

### Environment Configuration

Create a `.env` file from the template:

```bash
cp .env.example .env
```

Fill in:

- `SUPABASE_DB_URL`  
  Supabase â†’ Project Settings â†’ Database â†’ Connection string (URI)

- `OPENAI_API_KEY` *(optional)*  
  Enables real LLM streaming and summarization

Example:
```env
SUPABASE_DB_URL=postgresql://postgres:<password>@db.<project>.supabase.co:5432/postgres
OPENAI_API_KEY=sk-xxxx
OPENAI_MODEL=gpt-4o-mini
```

---

## 2ï¸. Supabase Database Schema

Run the SQL in `schema.sql` using **Supabase SQL Editor**.

### Tables

#### `sessions`
Stores high-level session metadata:
- `session_id`
- `user_id`
- `start_time`
- `end_time`
- `duration_seconds`
- `summary`

#### `session_events`
Stores a detailed chronological event log:
- user messages
- assistant responses
- system events
- interaction triggers

---

## 3ï¸. Run the Server

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Open:
- API root â†’ `http://localhost:8000/`
- Demo UI â†’ `http://localhost:8000/demo`

---

## 4ï¸. Testing the WebSocket

### Option A: Built-in Demo UI
1. Open `http://localhost:8000/demo`
2. Enter `session_id` and `user_id`
3. Click **Connect**
4. Send messages and observe token streaming

Example prompts:
- `Hello, explain what this system does`
- `What is my account balance?`
- `My user_id is user-1`
- `Check order status for order_id=ORD-1001`

---

### Option B: CLI (wscat)

```bash
npm install -g wscat
wscat -c ws://localhost:8000/ws/session/demo-session?user_id=user-1
```

Messages are streamed as JSON frames:
```json
{ "type": "start" }
{ "type": "token" }
{ "type": "done" }
```

---

## 5ï¸. Design Notes

### Realtime Streaming
- Responses are streamed incrementally over WebSocket
- Mimics true LLM token streaming behavior

### Complex Interaction
- Conversation state is maintained across turns
- Assistant may request missing parameters
- Tool / function-calling pattern is implemented and extensible

### Persistence
- All events are written to Supabase Postgres
- Enables replay, audit, and post-session analysis

### Post-Session Automation
- Triggered automatically on WebSocket close
- Runs asynchronously
- Generates and persists session summary and metadata

---

## ğŸ“ Repository Structure

```
tecnvi_realtime_ai_backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ llm.py
â”‚   â””â”€â”€ tools.py
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ schema.sql
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## Assignment Coverage Summary

| Requirement | Status |
|-----------|--------|
| Realtime WebSocket sessions 
| Token streaming 
| Complex LLM interaction 
| Supabase persistence 
| Post-session summarization 
| Simple frontend 

---

## Deployment (Optional)
This project is deployment-ready on platforms that support WebSockets, such as:
- Render
- Railway
- Fly.io

Environment variables must be configured in the deployment platform (do **not** commit `.env`).

---

### Security Note
`.env` files are intentionally excluded from version control.  
Use `.env.example` as a reference for required configuration.
